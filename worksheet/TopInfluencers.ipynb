{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nxpd in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.2.0)\n",
      "Requirement already satisfied: networkx>=1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nxpd) (2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nxpd) (2.2.0)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from networkx>=1.6->nxpd) (4.3.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nxpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting community\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7b/6d1d5efe9518b73b61466607acd66e8d0463b8085c56253974d2c23d4b48/community-1.0.0b1.tar.gz\n",
      "Requirement already satisfied: Flask in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from community) (1.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from community) (2.20.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Flask->community) (0.24)\n",
      "Requirement already satisfied: click>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Flask->community) (6.7)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Flask->community) (2.10)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Flask->community) (0.14.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->community) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->community) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->community) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->community) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Jinja2>=2.10->Flask->community) (1.0)\n",
      "Building wheels for collected packages: community\n",
      "  Running setup.py bdist_wheel for community ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/0d/7e/f9/089b3c72d33c16b8d3edb17f64291511e5eea08b80e419faa7\n",
      "Successfully built community\n",
      "Installing collected packages: community\n",
      "Successfully installed community-1.0.0b1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graph_tools\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/cb/af15d837a1089d34b840d91b45825080abf3ed4d9fa980b0fe766d97ac0d/graph_tools-1.5-py3-none-any.whl\n",
      "Collecting perlcompat (from graph_tools)\n",
      "  Downloading https://files.pythonhosted.org/packages/42/ab/328e564486228c8ed12085e4439fd39351e23333f52a8ed6833e4715efb8/perlcompat-1.1-py3-none-any.whl\n",
      "Collecting pytess (from graph_tools)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/f6/b687d1684d5db29e0a014dbafce6d0ec120353719631265c7f612bb41a4c/Pytess-1.0.0.tar.gz\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from graph_tools) (1.14.3)\n",
      "Building wheels for collected packages: pytess\n",
      "  Running setup.py bdist_wheel for pytess ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/e6/a9/0c/deb8003b037eb99504cb256ea963c6880bafd36e4d0cce6ce0\n",
      "Successfully built pytess\n",
      "Installing collected packages: perlcompat, pytess, graph-tools\n",
      "Successfully installed graph-tools-1.5 perlcompat-1.1 pytess-1.0.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graph_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as mc\n",
    "import networkx as nx\n",
    "import nxpd as nxpd\n",
    "import community as comm\n",
    "import graph_tools as gt\n",
    "from itertools import product\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'df_filtered.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create G\n",
    "def create_G(csv):\n",
    "    sparsemat = create_sparsity_matrix(csv)\n",
    "    features = sparsemat.columns\n",
    "    g_mat = np.array(sparsemat)\n",
    "    G = nx.Graph(g_mat)\n",
    "    return features, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, G = create_G(csv)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Gnodes(G):\n",
    "    G.graph['rankdir'] = 'LR'\n",
    "    return nxpd.draw(G, show='ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Gnodes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betweenness(G):\n",
    "    pos = nx.spring_layout(G)\n",
    "    betCent = nx.betweenness_centrality(G, normalized=True, endpoints=True)\n",
    "    node_color = [20000.0 * G.degree(v) for v in G]\n",
    "    node_size =  [v * 10000 for v in betCent.values()]\n",
    "    plt.figure(figsize=(20,20))\n",
    "    nx.draw_networkx(G, pos=pos, with_labels=False,\n",
    "                 node_color=node_color,\n",
    "                 node_size=node_size )\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5(G):\n",
    "    betCent = nx.betweenness_centrality(G, normalized=True, endpoints=True)\n",
    "    top5centrality = sorted(betCent, key=betCent.get, reverse=True)[:5]\n",
    "    return features[top5centrality]\n",
    "\n",
    "top5(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def reduce_by_critical_mass(G, crit_mass=5000):\n",
    "    \"\"\"\n",
    "    Reduces graph by removing nodes that have less than a specific number of\n",
    "        followers (critical mass, default=5000).\n",
    "    Args:\n",
    "        G (nx.Graph()): Graph object, with all influencers and followers.\n",
    "        crit_mass (Optional[int]): Critical mass following, default 5000.\n",
    "    Returns:\n",
    "        G_reduced (nx.Graph()): Graph object, containing only nodes with more\n",
    "            than critical mass of followers.\n",
    "    \"\"\"\n",
    "    G_reduced = G.copy()\n",
    "\n",
    "    G_nodes = set(G.nodes())\n",
    "    for node in G_nodes:\n",
    "        if len(G.in_edges(node)) < crit_mass:\n",
    "            G_reduced.remove_node(node)\n",
    "    return G_reduced\n",
    "\n",
    "def calc_community_influence_score(G,returned='list'):\n",
    "    \"\"\"\n",
    "    Calculate and sort by community influence scores, which is eigenvector centrality\n",
    "        for a graph (higher scores for higher eigenvector centralities).\n",
    "    Args:\n",
    "        G (nx.Graph()): Graph object.\n",
    "        returned [Optional(list)]: 'list' or 'dict' will change data structure returned.\n",
    "    Returns:\n",
    "        sorted_influence_score (list of tuples): List of nodes sorted by eigenvector\n",
    "            centrality.\n",
    "    \"\"\"\n",
    "\n",
    "    c = nx.eigenvector_centrality(G)\n",
    "    if returned == 'list':\n",
    "        sorted_influence_score = (sorted(c.items(), key=lambda x: x[1], reverse=True))\n",
    "        return sorted_influence_score\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "def calc_interaction_score(G, d, returned='list'):\n",
    "    \"\"\"\n",
    "    Calculate and sort by interaction scores for reduced graph based\n",
    "        on likes and followers (higher scores for larger likes to followers ratios).\n",
    "    Args:\n",
    "        G (nx.Graph()): Graph object.\n",
    "        d (dict): Dictionary with user ids, number of likes, and number of followers.\n",
    "        returned [Optional(list)]: 'list' or 'dict' will change data structure returned.\n",
    "    Returns:\n",
    "        sorted_interaction_score\n",
    "    \"\"\"\n",
    "    interaction_score = {}\n",
    "\n",
    "    for accession in G.nodes():\n",
    "#         if accession == '':\n",
    "#             interaction_score[''] = 0.05\n",
    "#         else:\n",
    "            interaction_score[accession] = (1.0*d[accession]['max_likes']/d[accession]['num_follow'])\n",
    "\n",
    "    if returned == 'list':\n",
    "        sorted_interaction_score = (sorted(interaction_score.iteritems(), key=lambda x: x[1], reverse=True))\n",
    "        return sorted_interaction_score\n",
    "    else:\n",
    "        return interaction_score\n",
    "\n",
    "def calc_authenticity_score(G, captions, returned='list'):\n",
    "    \"\"\"\n",
    "    Calculate and sort by authenticity scores for reduced graph based\n",
    "        on compound sentiment analysis score (higher scores for more negative posts).\n",
    "    Args:\n",
    "        G (nx.Graph()): Graph object.\n",
    "        captions (dict): Dictionary with user ids and captions.\n",
    "        returned [Optional(list)]: 'list' or 'dict' will change data structure returned.\n",
    "    Returns:\n",
    "        sorted_authenticity_score\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    top_influencers = G.nodes()\n",
    "    top_influencers.remove(\"SelenaGomez\")\n",
    "\n",
    "    caption_sentiment = {}\n",
    "    for user_id in top_influencers:\n",
    "        for caption in captions[user_id]['caption']:\n",
    "            vs = analyzer.polarity_scores(caption)\n",
    "            if user_id in caption_sentiment:\n",
    "                caption_sentiment[user_id].append(vs)\n",
    "            else:\n",
    "                caption_sentiment[user_id] = [vs]\n",
    "\n",
    "    caption_sentiment_means = {}\n",
    "    for user_id in caption_sentiment:\n",
    "        df = pd.DataFrame(caption_sentiment[user_id])\n",
    "        # Subtract compound score from 1 to penalize more positive captions\n",
    "        caption_sentiment_means[user_id] = 1 - dict(df.mean())['compound']\n",
    "    # Add Selena Gomez for test\n",
    "    caption_sentiment_means[\"SelenaGomez\"] = 0.5\n",
    "\n",
    "    if returned == 'list':\n",
    "        sorted_authenticity_score = (sorted(caption_sentiment_means.iteritems(), key=lambda x: x[1], reverse=True))\n",
    "        return sorted_authenticity_score\n",
    "    else:\n",
    "        return caption_sentiment_means\n",
    "\n",
    "def normalize_values(d):\n",
    "    \"\"\"\n",
    "    Return dictionary with normalized values (0 to 1).\n",
    "    Args:\n",
    "        d (dict): Dictionary with user ids and raw scores.\n",
    "    Returns:\n",
    "        d_norm (dict): Dictionary with user ids and normalized scores.\n",
    "    \"\"\"\n",
    "    minimum = min(d.iteritems(), key= lambda x: x[1])[1]\n",
    "    maximum = max(d.iteritems(), key= lambda x: x[1])[1]\n",
    "\n",
    "    d_norm = {}\n",
    "    for user_id in d:\n",
    "        d_norm[user_id] = (d[user_id] - minimum) / (maximum - minimum)\n",
    "\n",
    "    return d_norm\n",
    "\n",
    "def calc_overall_score(influence_score, interaction_score, authenticity_score):\n",
    "    \"\"\"\n",
    "    Calculate overall scores and save all types of scores to one dictionary.\n",
    "    Args:\n",
    "        influence_score (dict): Dictionary with user ids and influence scores.\n",
    "        interaction_score (dict): Dictionary with user ids and interaction scores.\n",
    "        authenticity_score (dict): Dictionary with user ids and authenticity scores.\n",
    "    Returns:\n",
    "        scores (dict): Dictionary with all score types, and final scores.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    users = influence_score.keys()\n",
    "\n",
    "    # Normalize scores dictionaries\n",
    "    influence_normed = normalize_values(influence_score)\n",
    "    interaction_normed = normalize_values(interaction_score)\n",
    "    authenticity_normed = normalize_values(authenticity_score)\n",
    "\n",
    "    # Add normalized scores types to scores\n",
    "    for user_id in users:\n",
    "        scores[user_id] = {'influence':None, 'interaction':None,\n",
    "                            'authenticity':None, 'final':None}\n",
    "        scores[user_id]['influence'] = influence_normed[user_id]\n",
    "        scores[user_id]['interaction'] = interaction_normed[user_id]\n",
    "        scores[user_id]['authenticity'] = authenticity_normed[user_id]\n",
    "\n",
    "        # Add overall score to scores\n",
    "        scores[user_id]['final'] = (0.6*influence_normed[user_id] +\n",
    "                            0.3*interaction_normed[user_id] +\n",
    "                            0.1*authenticity_normed[user_id])\n",
    "    return scores\n",
    "\n",
    "def find_top_influencers(G, filepath_interactions, filepath_captions):\n",
    "    \"\"\"\n",
    "    Find top influencers based on a graph of influencers and followers,\n",
    "    numbers of likes, and authenticity of posts.\n",
    "    Args:\n",
    "        G (nx.Graph()): Graph object, with all influencers and followers.\n",
    "        filepath_interactions (str): Filepath to json with interactions dictionary.\n",
    "        filepath_captions (str): Filepath to json with captions dictionary.\n",
    "    Returns:\n",
    "        scores (dict): Dictionary with all score types, and final scores.\n",
    "    \"\"\"\n",
    "    # Reduce by in-degree\n",
    "    G_reduced = reduce_by_critical_mass(G)\n",
    "    interactions_dict = load_json(filepath_interactions)\n",
    "    captions_dict = load_json(filepath_captions)\n",
    "\n",
    "    # Calculate different scores\n",
    "    influence_score = calc_community_influence_score(G_reduced, returned='dict')\n",
    "    interaction_score = calc_interaction_score(G_reduced, interactions_dict, returned='dict')\n",
    "    authenticity_score = calc_authenticity_score(G_reduced, captions_dict, returned='dict')\n",
    "\n",
    "    # Calculate final scores\n",
    "    scores = calc_overall_score(influence_score, interaction_score, authenticity_score)\n",
    "\n",
    "    return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
